package project2

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._


object loadToHDFS{
  val path = "hdfs://sandbox-hdp.hortonworks.com:8020/user/maria_dev/"
  val src = "file:///home/maria_dev/InsuranceClaims.csv"
  val spark: SparkSession = SparkSession
    .builder()
    .master("local[3]")
    .appName("Amplifire")
    .getOrCreate()
    spark.sparkContext.setLogLevel("ERROR")
  val sc = spark.sparkContext
  val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)

  def main(args: Array[String]):Unit = {
      toHive()
    }
  def copyFromLocal(): Unit = {
    val target = path + "InsuranceClaims.csv"
    println(s"Copying local file $src to $target ...")
    
    val conf = new Configuration()
    val fs = FileSystem.get(conf)

    val localpath = new Path(src)
    val hdfspath = new Path(target)
    
    fs.copyFromLocalFile(false, localpath, hdfspath)
    println(s"Done copying local file $src to $target ...")
  }
  def toHive(): Unit = {
      
      copyFromLocal() // moves the file into hdfs
      sqlContext.sql("""CREATE TABLE IF NOT EXISTS default.Insurance (
      claim_id string,
      customer_id string,
      customer_name string, 
      customer_age int,
      agent_id int,
      agent_name string,
      claim_category string,
      amount int,
      reason string,
      agent_rating int,
      datetime string,
      country string,
      state string,
      approval string,
      reimbursement_id string,
      failure_reason string)
      COMMENT 'Insurance Table'
      ROW FORMAT DELIMITED
      FIELDS TERMINATED BY ','""")
    
    sqlContext.sql("""LOAD DATA INPATH '/user/maria_dev/InsuranceClaims.csv' INTO TABLE default.Insurance""")
    //var data = sqlContext.sql("""SELECT * FROM default.Insurance""")
    }

def fromCSVFile(): Unit = {
    val df = spark.read
      .option("header", true)
      .option("inferSchema", true)
      .csv(src)
}
}
